<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>NWB in LinkML - Lab Meeting 2024-09-16</title>

		<link rel="stylesheet" href="reset.css">
		<link rel="stylesheet" href="index.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
<section data-markdown>
	<textarea data-template>
		# NWB in Linkml

		## Background

		## Implementation

		## Numpydantic

		## The Future...
	</textarea>
</section>
<section>
	<h2>What do we want from a data format?</h2>
	<p>Yeah sure, FAIR, but specifically...</p>
	<div class="row">
		<div class="column half">
			<h3>For Producers</h3>
			<ul class="fragment">
				<li>Easy to understand</li>
				<li>Easy to convert to</li>
				<li>Integrates with existing practices</li>
				<li>Extensible: accomodates all experimental details</li>
				<li>Evolvable: versionable, iteratively add multiple phases of experiment</li>
				<li>Low-risk: difficult to lose data from e.g. corruption</li>
				<li>Efficient: doesn't duplicate data, uses compression kindly</li>
			</ul>
		</div>
		<div class="column half">
			<h3>For Consumers</h3>
			<ul class="fragment">
				<li>Easy to find and index</li>
				<li>Uses standardized vocabularies</li>
				<li>Interoperable with other formats</li>
				<li>High-performance: query large amounts of data</li>
				<li>Good UX for common data types: arrays, tables, etc.</li>
				<li>Streamable: get only the data you want</li>
			</ul>
		</div>
	</div>

</section>
<section>
	<section>
		<h2>Hierarchical Data Formats</h2>
		<p>eg. BIDS, NWB</p>
		<div class="row">
			<div class="column half">
				<h3>Good</h3>
				<ul>
					<li>Groups similar things together</li>
					<li>Filesystem is a familiar metaphor</li>
					<li>Everything has a single location</li>
					<li>Format-general</li>
				</ul>
			</div>
			<div class="column half">
				<h3>Bad</h3>
				<ul>
					<li>References are hard</li>
					<li>Reuse is hard</li>
					<li>Everything has a single location</li>
					<li>Not everything is a hierarchy</li>
					<li>Deep nesting is cumbersome</li>
				</ul>
			</div>
		</div>


	</section>

	<section>
		<h3>BIDS</h3>
		<div class="row">
			<div class="column third">
				<p>Primarily used in neuroimaging, but also does EEG, fNIRS, etc.</p>
				<ul>
					<li>Hierarchical</li>
					<li>Filesystem-Based: filenames encode type and metadata</li>
					<li>Fixed set of types, or <a data-preview-link href="https://bids-standard.github.io/bids-starter-kit/folders_and_files/files.html#modalities">"modalities"</a> </li>
					<li>Metadata in json + tsv files</li>
					<li>Loose coupling with raw data: use whatever format you want, no guarantees on support</li>
					<li>Single-use, JSON-Schema-like "<a href="https://github.com/bids-standard/bids-specification/tree/master/src/schema">bids schema language</a>"</li>
				</ul>
			</div>
			<div class="column twothirds r-stack">
				<div class="fragment fade-out">
				<pre><code data-trim data-noescape>
ds001
├── dataset_description.json
├── participants.tsv
├── sub-01
│   ├── anat
│   │   ├── sub-01_inplaneT2.nii.gz
│   │   └── sub-01_T1w.nii.gz
│   └── func
│       ├── sub-01_task-balloonanalogrisktask_run-01_bold.nii.gz
│       ├── sub-01_task-balloonanalogrisktask_run-01_events.tsv
│       ├── sub-01_task-balloonanalogrisktask_run-02_bold.nii.gz
│       ├── sub-01_task-balloonanalogrisktask_run-02_events.tsv
├── sub-02
│   ├── anat
│   ...
				</code></pre>
				<pre><span class="label">dataset_description.json</span><code data-trim data-noescape>
{
  "Name": "",
  "BIDSVersion": "1.8.0",
  "HEDVersion": "8.2.0",
  "DatasetType": "raw",
	"DatasetDOI": "doi:"
	"...":"...",
}
					</code></pre></div>
				<div class="fragment">
					<pre><span class="label">participants.json</span><code data-trim >
{
  "age": {
    "Description": "age of the participant",
    "Units": "years",
    "TermURL": "https://www.todo.com/fixme",
    "HED": "Age/#"
  },
  "sex": {
    "Description": "sex of the participant as reported by the participant",
    "Levels": {
      "m": "male",
      "f": "female"
    },
    "TermURL": "https://www.todo.com/fixme"
  },
}
					</code></pre>
					<pre><span class="label">participants.tsv</span><code data-trim>
participant_id	age	sex	handedness
sub-01	        0		m		l
sub-epilepsy01	10	f		r
					</code></pre>
				</div>
			</div>
		</div>
	</section>
</section>
<section>
	<section>
	<h2>Relational</h2>
	<p>eg. DataJoint, Spyglass</p>
	<div class="row">
		<div class="column half">
			<h3>Good</h3>
			<ul>
				<li>Expressive</li>
				<li>Performant</li>
			</ul>
		</div>
		<div class="column half">
			<h3>Bad</h3>
			<ul>
				<li>Brittle</li>
				<li>Strict</li>
				<li>Monolithic (or require hosting an endpoint and API)</li>
				<li>Usually bad handling of arrays</li>
				<li>Complicated query system</li>
			</ul>
		</div>
	</div>
	</section>
	<section>
		<h2>Datajoint Example</h2>
		<div class="mermaid">
			<pre>
			erDiagram
				Recording {
					int device FK
					int acquisition_software FK
				}
				RecordingInfo {
					int recording FK
					int nchannels
					int nframes
					int px_height
					int px_width
					float fps
					str file FK
				}
				File {
					string file_path
				}
				Device {
				}
				AcquisitionSoftware {
				}
				ProcessingParams {
					int param_1
					int etc
				}
				Processing{
					int param_set FK
					int recording_info FK
				}
				Segmentation {
					int mask_npix
					int mask_center_x
					int mask_center_y
					int mask_xpix
					int mask_ypix
				}
				Fluorescence {
					blob fluorescence
					blob neuropil_fluorescence
				}


				Recording ||--|{ RecordingInfo: recording_info
				RecordingInfo ||--|| File: file
				Recording ||--|| AcquisitionSoftware: acquisition_software
				Recording ||--|| Device: device
				RecordingInfo ||--|{ Processing: processing
				ProcessingParams ||--|{ Processing: params
				Processing ||--|| Segmentation: segmentation
				Segmentation ||--|| Fluorescence: fluorescence
			</pre>
		</div>
	</section>
</section>
<section>
	<h2>Graph-Like</h2>
	<p>Usually RDF. Property, not class-centric.</p>
	<div class="row">
		<div class="column half">
			<h3>Good</h3>
			<ul>
				<li>Extremely expressive</li>
				<li>Easily refactorable</li>
				<li>Interoperability by design</li>
				<li>Rich and reusable metadata</li>
			</ul>
		</div>
		<div class="column half">
			<h3>Bad</h3>
			<ul>
				<li>Complex as all hell</li>
				<li>Handles common data structures like lists and arrays very badly</li>
				<li>Doesn't map neatly onto familiar patterns like object-oriented, filesystems, etc.</li>
				<li>The tooling sucks</li>
				<li>Lots of historical baggage</li>
			</ul>
		</div>
	</div>
</section>
<section class="center">
	<h2>Can we get the best of all worlds?</h2>
	<ul>
		<li>As familiar as files</li>
		<li>As performant as relational databases</li>
		<li>As expressive as RDF-like-things</li>
	</ul>
	<h2 class="fragment emphasis">We're sure as hell gonna try</h2>
</section>
<section>
	<h2><code>nwb-linkml</code> overview</h2>
	<div class="fullscreen">
		<img src="img/nwb-linkml-diagram.svg">
	</div>
</section>
<section data-markdown>
	<textarea data-template>
		# Trials and tribulations

		- resolving imports
		- rolling down classes
		- nonlocal/multilevel class effects
		- emulating all the hdmf special cases
		- resolving intra-file references

	</textarea>
</section>
<section>
	<section>
	<div class="row">
		<div class="column half">
			<h2>problem 1: NWB doesn't specify version dependencies.</h2>
			<p>
				The only place this information is found is in the git submodule version of the nwb-core repo<br>
				pynwb also <a href="https://github.com/hdmf-dev/hdmf/issues/1098">breaks schema versions</a> because of the
				<a href="https://github.com/NeurodataWithoutBorders/pynwb/pull/1931">brittle global namespace system</a>
			</p>
			<pre>
				<code data-trim data-line-numbers="5,6" class="language-yaml">
namespaces:
- name: core
# ...
  schema:
	# who is she
	- namespace: hdmf-common
	- doc: This source module contains base data types used throughout the NWB data format.
    source: nwb.base.yaml
    title: Base data types
				</code>
			</pre>
		</div>
		<div class="column half fragment">
			So we make an interface to git
			<pre><code data-trim>
>>> repo = GitRepo(NWB_CORE_REPO)
>>> repo.clone()
>>> # Check out a tag specifically
>>> repo.tag = "2.6.0"
>>> repo.tag
'2.6.0'
>>> # Now check out a commit some number after the tag.
>>> repo.commit = "ec0a879"
>>> repo.tag
'2.6.0-5-gec0a879'
>>> # Get the namespace file
>>> repo.namespace_file
'/var/tmp/some_hash/nwb_linkml__cache/git/core/nwb.namespace.yaml'
			</code></pre>
			And since parsing yaml is expensive, we peek with regex for the version...
<pre><code data-trim data-noescape>
def yaml_peek(
		key: str, path: Union[str, Path], root: bool = True, first: bool = True
) -> Union[str, List[str]]:
		if root:
        pattern = re.compile(rf"^(?P<key>{key}):\s*(?P<value>\S.*)")
    else:
        pattern = re.compile(rf"^\s*(?P<key>{key}):\s*(?P<value>\S.*)")
		# ...
		with open(path) as yfile:
				for line in yfile:
						res = pattern.match(line)
						if res:
								break
        if res:
            return res.groupdict()["value"]
</code></pre>
		</div>
	</div>
	</section>
	<section>
		<div class="row">
			<div class="column half">
				<h2>Caching by Virtualizing Imports</h2>
				<p>
					pynwb is tied to a single version of <code>nwb-schema</code>.<br>
					We want to provide all versions of all schemas.<br>
					Coincidentally, we also need to cache a lot.<br>
					So we use .env to make a relocatable cache...
				<pre>
__tmp__
├── git
│   ├── core
│   └── hdmf-common
├── linkml
│   ├── core
│   │   ├── ...
│   │   ├── v2_6_0_alpha
│   │   └── v2_7_0
│   ├── hdmf_common
│   │   ├── v1_5_0
│   │   └── v1_8_0
├── logs
└── pydantic
    ├── core
    │   ├── ...
    │   ├── v2_6_0_alpha
    │   └── v2_7_0
    ├── hdmf_common
    │   ├── ...
    │   ├── v1_5_0
    │   └── v1_8_0
		</pre>

				</p>
			</div>
			<div class="column half">
				And <a href="https://docs.python.org/3/library/importlib.html#importlib.abc.MetaPathFinder">patch into the python import machinery</a> so a model can be imported from the cache as if it's a normal python module
				<pre><code data-trim>
from importlib.abc import MetaPathFinder

class EctopicModelFinder(MetaPathFinder):
		MODEL_STEM = "nwb_models.models.pydantic"

		def __init__(self, path: Path, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.path = path

		def find_spec(
        self, fullname: str,
				path: Optional[str],
				target: Optional[ModuleType] = None
    ) -> Optional[ModuleSpec]:
        """
				If we are loading a generated pydantic module,
				return an importlib spec
				"""
        if not fullname.startswith(self.MODEL_STEM):
            return None
        else:
            # get submodule beneath model stem
            submod = fullname.replace(self.MODEL_STEM, "").lstrip(".")
            base_path = Path(self.path, *submod.split("."))

				# ...
				spec = importlib.util.spec_from_file_location(fullname, import_path)
				return spec
			</code></pre>
			</div>
		</div>
	</section>
</section>
<section>
	<div class="row">
		<div class="column half">
			<h2>Brief Refresher on NWB structure</h2>
			<p>NWB schemas have...</p>
			<ul>
				<li><strong>namespaces</strong> - collections of multiple schema files, each of which contains...</li>
				<li><strong>groups</strong> - which can contain groups, datasets, an attributes</li>
				<li><strong>datasets</strong> - which are the "leaves" of a schema, actually specifying array data</li>
				<li><strong>attributes</strong> - simple scalar metadata values</li>
			</ul>

		</div>
		<div class="column half">
			<p>eg. the TimeSeries group (abbreviated)</p>
			<pre><code class="language-yaml" data-line-numbers="4-6|8-17|13-17|18-27|28-30" data-trim>
- neurodata_type_def: TimeSeries
  neurodata_type_inc: NWBDataInterface
  doc: General purpose time series.
  attributes:
  - name: description
    dtype: text
  datasets:
  - name: data
    dims:
    - num_times
    shape:
    - null
    attributes:
    - name: conversion
      dtype: float32
    - name: unit
      dtype: text
  - name: timestamps
    dtype: float64
    dims:
    - num_times
    shape:
    - null
    quantity: '?'
    attributes:
    - name: interval
    - name: unit
	group:
  - name: sync
    quantity: '?'

			</code></pre>
		</div>
	</div>
</section>

<section>
	<section>
	<div class="row">
		<div class="column half">
			<h2>problem 2: nwb's inheritance is whacky</h2>
			<p>NWB's inheritance system mixes inheritance with object instantiation...</p>
			<table class="docutils align-default smaller">
				<thead>
				<tr class="row-odd"><th class="head"><p><code class="docutils literal notranslate"><span class="pre">type_inc</span></code></p></th>
					<th class="head"><p><code class="docutils literal notranslate"><span class="pre">type_def</span></code></p></th>
					<th class="head"><p>Description</p></th>
				</tr>
				</thead>
				<tbody>
				<tr class="row-even"><td><p>not set</p></td>
					<td><p>not set</p></td>
					<td><p>define a standard dataset or group without a type</p></td>
				</tr>
				<tr class="row-odd"><td><p>not set</p></td>
					<td><p>set</p></td>
					<td><p>create a new data type from scratch</p></td>
				</tr>
				<tr class="row-even"><td><p>set</p></td>
					<td><p>not set</p></td>
					<td><p>include (reuse) data type without creating a new one (include)</p></td>
				</tr>
				<tr class="row-odd"><td><p>set</p></td>
					<td><p>set</p></td>
					<td><p>merge/extend data type and create a new type (inheritance/merge)</p></td>
				</tr>
				</tbody>
			</table>
		</div>
		<div class="column half">
			<p>And inheritance is <a href="https://github.com/NeurodataWithoutBorders/pynwb/issues/1954#issuecomment-2327718036">recursively merged</a>, rather than simply overridden</p>
			<pre><code data-trim class="language-python" data-line-numbers>
@dataclass
class TimeSeries:
    data: 'Data'
    timestamps: list[float]

    @dataclass
    class Data:
        conversion: float = 1.0
        offset: float = 0.0
        resolution: float = -1.0
        unit: str

@dataclass
class VoltageClampStimulusSeries(TimeSeries):
    data: 'Data'
    # we expect to have the rest of the top-level fields
		# like timestamps here from inheritance

    @dataclass
    class Data:
        unit: str = "float"
        # because `Data` is defined locally
				# within TimeSeries and VoltageClampStimulusSeries,
        # we wouldn't expect to inherit `conversion`, etc.
				# props, and yet in nwb we do.
			</code></pre>
		</div>
	</div>
	</section>
	<section>
		<div class="row">
			<div class="column third">
				<p>Thankfully we made reasonable (linkml) data models to represent nwb-schema-language</p>
				<pre><code data-trim class="language-python">
class Group(ConfiguredBaseModel):

    neurodata_type_def: Optional[str] = Field(
        None,
        description="""Used alongside neurodata_type_inc to indicate inheritance, naming, and mixins""",
    )
    neurodata_type_inc: Optional[str] = Field(
        None,
        description="""Used alongside neurodata_type_def to indicate inheritance, naming, and mixins""",
    )
    name: Optional[str] = Field(None)
    default_name: Optional[str] = Field(None)
    doc: str = Field(..., description="""Description of corresponding object.""")
    quantity: Optional[Union[QuantityEnum, int]] = Field(1)
    linkable: Optional[bool] = Field(None)
    attributes: Optional[List[Attribute]] = Field(default_factory=list)
    datasets: Optional[List[Dataset]] = Field(default_factory=list)
    groups: Optional[List[Group]] = Field(default_factory=list)
    links: Optional[List[Link]] = Field(default_factory=list)
				</code></pre>
			</div>
			<div class="column twothirds">
				<p>So we can recursively roll down inheriting class props</p>
				<pre><code data-trim class="language-python" data-line-numbers="5-6|8-10|11-17|18-30|19-21|22-30">
def roll_down_nwb_class(
	source: Group | Dataset | dict, target: Group | Dataset | dict,
	complete: bool = False
) -> dict:
	# don't copy every property, just those that the child touches
	ret = {k: v for k, v in source.items() if k not in exclude and k in target}
	for key, value in target.items():
		if key not in ret:
			# straight through if the child overwrites a scalar value
			ret[key] = value
		elif isinstance(value, dict):
			# merge if the child is itself a dataset, group, or attr
			# children get `complete`ly merged, while top-level classes don't
			if key in ret:
				ret[key] = roll_down_nwb_class(ret[key], value, complete=True)
			else:
				ret[key] = value
		elif isinstance(value, list) and all([isinstance(v, dict) for v in value]):
			# we have to match lists of datasets/groups by the name key...
			src_keys = {v["name"]: ret[key].index(v) for v in ret.get(key, {}) if "name" in v}
			target_keys = {v["name"]: value.index(v) for v in value if "name" in v}
			# ... merge dicts in both source and target
			new_val.extend(
				[
					roll_down_nwb_class(ret[key][src_keys[k]], value[target_keys[k]], complete=True)
					for k in target_keys
					if k in set(src_keys.keys()).intersection(set(target_keys.keys()))
				]
			)
			ret[key] = new_val
		else:
			ret[key] = value
				</code></pre>
			</div>
		</div>
	</section>
	<section>
		<p>(in pynwb this is a manual process, and it often defies the schema)</p>
		<p>note how unit is overridden for VoltageClampStimulusSeries.attributes, <br>
		when in the schema it is overridden for VoltageClampStimulusSeries.data.attributes</p>
		<div class="row">
			<div class="column half"><pre><code data-trim class="language-yaml">
- neurodata_type_def: VoltageClampStimulusSeries
  neurodata_type_inc: PatchClampSeries
  doc: Stimulus voltage applied during a voltage clamp recording.
  datasets:
  - name: data
    doc: Stimulus voltage applied.
    attributes:
    - name: unit
      dtype: text
      value: volts
      doc: Base unit of measurement for working with the data. which is fixed to 'volts'.
        Actual stored values are not necessarily stored in these units. To access the data in these units,
        multiply 'data' by 'conversion' and add 'offset'.
			</code></pre></div>
			<div class="column half">
				<pre><code data-trim class="language-python">
@register_class('VoltageClampStimulusSeries', CORE_NAMESPACE)
class VoltageClampStimulusSeries(PatchClampSeries):
    '''
    Alias to standard PatchClampSeries. Its functionality is to better tag PatchClampSeries for
    machine (and human) readability of the file.
    '''

    __nwbfields__ = ()

    @docval(*get_docval(PatchClampSeries.__init__, 'name', 'data', 'electrode', 'gain'),  # required
			{
				'name': 'unit',
			  'type': str,
			  'doc': "The base unit of measurement (must be 'volts')",
			  'default': 'volts'
			}
		)
    def __init__(self, **kwargs):
        name, data, unit, electrode, gain = popargs('name', 'data', 'unit', 'electrode', 'gain', kwargs)
        unit = ensure_unit(self, name, unit, 'volts', '2.1.0')
        super().__init__(name, data, unit, electrode, gain, **kwargs)
				</code></pre>
			</div>
		</div>
	</section>
</section>
<section>
	<section>
	<h2>problem 3: nwb -> linkml translation is nonlocal in nwb</h2>
	<p>We want to create simple models, so...</p>
	<ul>
		<li>Yield a new class when a dataset has additional attributes</li>
		<li>Yield only a model field it captures the full dataset description</li>
	</ul>
	<div class="row">
		<div class="column half">
<pre><code data-trim class="language-yaml">
groups:
- neurodata_type_def: ImageSeries
  neurodata_type_inc: TimeSeries
  datasets:
  - name: data
    dtype: numeric
    dims:
    - - frame
      - x
      - y
    shape:
    - - null
      - null
      - null
  - name: external_file
    dtype: text
    dims:
    - num_files
    shape:
    - null
    attributes:
    - name: starting_frame
      dtype: int32
</code></pre>
		</div>
		<div class="column half">
			<pre><code class="language-python">
class ImageSeries(TimeSeries):
	data: NDArray[Shape["* frame, * x, * y"], float]
	external_file: Optional[ImageSeriesExternalFile] = None

class ImageSeriesExternalFile(ConfiguredBaseModel):
	value: Optional[NDArray[Shape["* num_files"], str]] = None
	starting_frame: List[int]

			</code></pre>
		</div>
	</div>
	</section>
	<section>

		<div class="row">
			<div class="column third">
				<h2>So we make a nonlocal build system</h2>
				<ul>
					<li>Make a container for build results</li>
					<li>Build stages check for applicability</li>
					<li>Then receive and return the modified build result</li>
					<li>for ... bidirectional recursion?</li>
				</ul>
				<pre><code data-trim class="language-python">
@dataclass
class BuildResult:
    """
    Container class for propagating nested build results back up to caller
    """

    schemas: List[SchemaDefinition]
    classes: List[ClassDefinition]
    slots: List[SlotDefinition]
    types: List[TypeDefinition]
				</code></pre>
			</div>
			<div class="column twothirds">
				<pre><code data-trim class="language-python" data-line-numbers="8-16|24-27">
class MapArraylike(DatasetMap):
	def apply(
		c, cls: Dataset, res: Optional[BuildResult] = None, name: Optional[str] = None
	) -> BuildResult:
		array_adapter = ArrayAdapter(cls.dims, cls.shape)
		expressions = array_adapter.make_slot()
		name = camel_to_snake(cls.name)
		res = BuildResult(
			slots=[
				SlotDefinition(
						name=name,
						range=handle_dtype(cls.dtype),
						required=cls.quantity not in ("*", "?"),
						**expressions,
					)])
		return res

class MapArrayLikeAttributes(DatasetMap):
	def apply(
		c, cls: Dataset, res: Optional[BuildResult] = None, name: Optional[str] = None
	) -> BuildResult:
		array_adapter = ArrayAdapter(cls.dims, cls.shape)
		expressions = array_adapter.make_slot()
		array_slot = SlotDefinition(
				name="value", range=handle_dtype(cls.dtype), inlined=inlined(cls.dtype), **expressions
		)
		res.classes[0].attributes.update({"value": array_slot})
		return res
				</code>
				</pre>
			</div>
		</div>
	</section>
	<section>
Where every class builds a class for itself (if needed) and a slot that
its parent uses to refer to it
<pre><code class="language-python" data-trim>
def build_base(self) -> BuildResult:
	# ...
	cls = ClassDefinition(**kwargs)

	slots = []
	if self.parent is not None:
			slots.append(self.build_self_slot())

	return BuildResult(classes=[cls], slots=slots)
</code></pre>
		Then merge the results as they ascend the recursive chain with an addition metaphor
		<pre><code class="language-python" data-trim>
Class GroupAdapter(ClassAdapter):
	def build_subclasses(self) -> BuildResult:
		"""
		Build nested groups and datasets

		Create ClassDefinitions for each, but then also create SlotDefinitions that
		will be used as attributes linking the main class to the subclasses
		"""
		# Datasets are simple, they are terminal classes, and all logic
		# for creating slots vs. classes is handled by the adapter class
		dataset_res = BuildResult()
		for dset in self.cls.datasets:
				dset_adapter = DatasetAdapter(cls=dset, parent=self)
				dataset_res += dset_adapter.build()

		</code></pre>

	</section>
</section>
<section class="center">
	<p>so great, we have schemas, what's the ...</p>

	<p class="fragment">o shit is that....</p>

	<p class="fragment emphasis">enough special cases <br>to blot out the sun</p>

	<div class="row">
		<div class="column half"></div>
		<div class="column half"></div>
	</div>
</section>
<section>
	<section>
	<div class="r-stack">
		<div>
		<h2>problem 4: most of nwb is implemented as special-cased classes that only exist in words in the schema</h2>
		So ya got
		<ul>
			<li><strong>DynamicTable:</strong> an unbounded set of user-specifiable, extra-schema columns, which are...</li>
			<li><strong>VectorData:</strong> A column of data within a DynamicTable that can also have a...</li>
			<li><strong>VectorIndex:</strong> Another column of indices that turn a VectorData column into a ragged-length array</li>
			<li><strong>DynamicTableRegion:</strong> A subclass of VectorData with a reference to another table, which makes it both a vector of table rows AND a ragged array of table rows</li>
			<li><strong>AlignedDynamicTable:</strong> A subclass of DynamicTable which can contain multiple tables with matching <code>id</code> columns</code></li>
			<li><strong>TimeSeriesReferenceVectorData:</strong> A subclass of VectorData that can refer to arbitrary ranges within a TimeSeries dataset, which can have a VectorIndex making it a ragged array of arbitrary ranges of a TimeSeries</li>
		</ul>
		</div>
		<img class="fragment" src="img/jesus.png">
	</div>
	</section>
	<section>
		<h2>Lifecycle methods to the rescue</h2>
		<p>The linkml pydantic generator lets you modify generation per class and slot</p>
		<pre><code data-trim class="language-python">
for cls in source_classes:
	cls = self.before_generate_class(cls, sv)
	result = self.generate_class(cls)
	result = self.after_generate_class(result, sv)
		</code></pre>
		<p>So we can cleanly partition the HDMF special cases into <a href="https://github.com/p2p-ld/nwb-linkml/blob/main/nwb_linkml/src/nwb_linkml/includes/hdmf.py">a set of mixin classes</a></p>
		<pre><code data-trim class="language-python">
class VectorDataMixin(BaseModel, Generic[T]):
	def __getitem__(self, item: Union[str, int, slice, Tuple[Union[str, int, slice], ...]]) -> Any:
	def __setitem__(self, key: Union[int, str, slice], value: Any) -> None:

class VectorIndexMixin(BaseModel, Generic[T]):
	def __getitem__(self, item: Union[int, slice, Iterable]) -> Any:
	def __setitem__(self, key: Union[int, slice], value: Any) -> None:
		</code></pre>
		<p>And inject them during the generation process</p>
		<pre><code data-trim class="language-python">
def after_generate_class(self, cls: ClassResult, sv: SchemaView) -> ClassResult:
		"""Customize dynamictable behavior"""
		cls = AfterGenerateClass.inject_dynamictable(cls)
		cls = AfterGenerateClass.wrap_dynamictable_columns(cls, sv)
		cls = AfterGenerateClass.inject_elementidentifiers(cls, sv, self._get_element_import)
		cls = AfterGenerateClass.strip_vector_data_slots(cls, sv)
		return cls
		</code></pre>

	</section>
	<section>
		<p>Where we can emulate the behavior of hdmf accurately <3</p>
		<pre><code data-trim class="language-python">
>>> intracellular_recordings_table[0:5]
   electrodes                                                    stimuli                              responses
           id                                          electrode      id                     stimulus        id                  response
id
0           0  hdf5_path=None object_id=None name='my_electro...       0  [0, 0, 0, 0, 0, 0, 0, 0, 0]         0                       [0]
1           1  hdf5_path=None object_id=None name='my_electro...       1     [1, 1, 1, 1, 1, 1, 1, 1]         1                    [1, 1]
2           2  hdf5_path=None object_id=None name='my_electro...       2  [2, 2, 2, 2, 2, 2, 2, 2, 2]         2  [2, 2, 2, 2, 2, 2, 2, 2]
3           3  hdf5_path=None object_id=None name='my_electro...       3              [3, 3, 3, 3, 3]         3           [3, 3, 3, 3, 3]
4           4  hdf5_path=None object_id=None name='my_electro...       4                       [4, 4]         4                    [4, 4]

		</code></pre>
		<p>And handle references by substituting the object directly</p>
		<pre><code>
>>> intracellular_recordings_table[0:5].electrodes.electrode[0]
IntracellularElectrode(
			hdf5_path=None,
			object_id=None,
			name='my_electrode',
			cell_id=None,
			description='an electrode',
			filtering=None,
			initial_access_resistance=None,
			location=None,
			resistance=None,
			seal=None,
			slice=None,
			device=Device(hdf5_path=None, object_id=None, name='my device', description=None, manufacturer=None))

		</code></pre>
	</section>
</section>
<section>
	<section>
	<h2>Numpydantic - it's got what arrays crave</h2>
	<p>How do we generalize the specification of an array to not be dependent on any one format?</p>
	<pre><code data-trim class="language-python">
class TimeSeriesData(ConfiguredBaseModel):
    """
    Data values. Data can be in 1-D, 2-D, 3-D, or 4-D. The first dimension should always represent time. This can also be used to store binary data (e.g., image frames). This can also be a link to data stored in an external file.
    """
		value: Optional[
			Union[
					NDArray[Shape["* num_times"], Any],
					NDArray[Shape["* num_times, * num_dim2"], Any],
					NDArray[Shape["* num_times, * num_dim2, * num_dim3"], Any],
					NDArray[Shape["* num_times, * num_dim2, * num_dim3, * num_dim4"], Any],
			]
    ] = Field(None)
	</code></pre>
	</section>
	<section>
		<h2>Overview</h2>
		<p>Interfaces provide an abstract coercion and type checking system</p>
		<div class="mermaid">
			<pre>
%%{init: {'theme': 'dark', 'themeVariables': { 'darkMode': true }}}%%
flowchart LR
	classDef data fill:#2b8cee,color:#ffffff;
	classDef X fill:transparent,border:none,color:#ff0000;

	input

	subgraph Interface
	match
	end

	subgraph Numpy
	numpy_check["check"]
	end

	subgraph Dask
	direction TB

	dask_check["check"]

	subgraph Validation
	direction TB

	before_validation --> validate_dtype
	validate_dtype --> validate_shape
	validate_shape --> after_validation
	end

	dask_check --> Validation

	end

	subgraph Zarr
	zarr_check["check"]
	end

	subgraph Model
	output
	end

	zarr_x["X"]
	numpy_x["X"]

	input --> match
	match --> numpy_check
	match --> zarr_check
	match --> Dask
	zarr_check --> zarr_x
	numpy_check --> numpy_x

	Validation --> Model

	class input data
	class output data
	class zarr_x X
	class numpy_x X
			</pre>
		</div>
	</section>
	<section>
		<h2>Using the pydantic validation system</h2>
		<pre><code data-trim class="language-python">
class NDArray(NPTypingType, metaclass=NDArrayMeta):
		@classmethod
    def __get_pydantic_core_schema__(
        cls,
        _source_type: "NDArray",
        _handler: _handler_type,
    ) -> core_schema.CoreSchema:
        shape, dtype = _source_type.__args__
        shape: ShapeType
        dtype: DtypeType

        # get pydantic core schema as a list of lists for JSON schema
        list_schema = make_json_schema(shape, dtype, _handler)

        return core_schema.json_or_python_schema(
            json_schema=list_schema,
            python_schema=core_schema.with_info_plain_validator_function(
                get_validate_interface(shape, dtype)
            ),
            serialization=core_schema.plain_serializer_function_ser_schema(
                _jsonize_array, when_used="json", info_arg=True
            ),
        )
		</code></pre>
	</section>
	<section>
		<p>Each NDArray type has a closure to find validators at runtime</p>
		<pre><code>
def get_validate_interface(shape: ShapeType, dtype: DtypeType) -> Callable:
    """
    Validate using a matching :class:`.Interface` class using its
    :meth:`.Interface.validate` method
    """

    def validate_interface(
        value: Any, info: Optional["ValidationInfo"] = None
    ) -> NDArrayType:
        interface_cls = Interface.match(value)
        interface = interface_cls(shape, dtype)
        value = interface.validate(value)
        return value

    return validate_interface
		</code></pre>

	</section>
	<section>
		<p>And an interface follows a familiar lifecycle hook pattern...</p>
		<pre><code class="language-python">
class Interface(ABC, Generic[T]):
		input_types: Tuple[Any, ...]
    return_type: Type[T]

		def validate(self, array: Any) -> T:
			  array = self.before_validation(array)

        dtype = self.get_dtype(array)
        dtype_valid = self.validate_dtype(dtype)
        self.raise_for_dtype(dtype_valid, dtype)
        array = self.after_validate_dtype(array)

        shape = self.get_shape(array)
        shape_valid = self.validate_shape(shape)
        self.raise_for_shape(shape_valid, shape)

        array = self.after_validation(array)
        return array</code></pre>
	</section>
	<section>
		<h2>The ecstasy of generality</h2>
		<pre><code data-trim class="language-python">
from pydantic import BaseModel
from numpydantic import NDArray, Shape
import numpy as np
import dask.array as da
import zarr

class MyModel(BaseModel):
    array: NDArray[Shape["3 x, 4 y, * z"], int]

# numpy
model = MyModel(array=np.zeros((3, 4, 5), dtype=int))
# dask
model = MyModel(array=da.zeros((3, 4, 5), dtype=int))
# hdf5 datasets
model = MyModel(array=('data.h5', '/nested/dataset'))
# zarr arrays
model = MyModel(array=zarr.zeros((3,4,5), dtype=int))
model = MyModel(array='data.zarr')
model = MyModel(array=('data.zarr', '/nested/dataset'))
# video files
model = MyModel(array="data.mp4")
		</code></pre>
	</section>
</section>
<section>
	<section>
	<h2>A Conversion-less data format</h2>
	<p>What if i told you that this was totally unnecessary...</p>
	<div class="row">
		<div class="column third"><pre>
to_nwb
├── IBL-to-nwb
├── MICrONS-to-nwb
├── ahrens-lab-to-nwb
├── allen-oephys-to-nwb
├── axel-lab-to-nwb
├── brody-lab-to-nwb
├── brunton-lab-to-nwb
├── buffalo-lab-to-nwb
├── buzsaki-lab-to-nwb
├── clandinin-lab-to-nwb
├── datta-lab-to-nwb
├── dombeck-lab-to-nwb
├── fee-lab-to-nwb
├── feldman-lab-to-nwb
├── froemke-lab-to-nwb
├── giocomo-lab-to-nwb
├── higley-lab-to-nwb
├── hussaini-lab-to-nwb
├── jaeger-lab-to-nwb
├── jazayeri-lab-to-nwb
├── ...
		</pre></div>
		<div class="column twothirds"><pre>
$ cloc to_nwb
		1227 text files.
    1031 unique files.
     276 files ignored.

github.com/AlDanial/cloc v 2.02  T=1.37 s (752.6 files/s, 104684.6 lines/s)
--------------------------------------------------------------------------------
Language                      files          blank        comment           code
--------------------------------------------------------------------------------
Python                          629          11254          10842          50912
Jupyter Notebook                 64              0          42912           7635
Text                             70             19              0           4818
YAML                            158            128            131           4636
Markdown                         78           1339              0           4294
JSON                             10             21              0           3842
TOML                             16             21              0            421
CSV                               2              0              0             94
Bourne Again Shell                1              5              0             19
MATLAB                            1              6              8             18
Bourne Shell                      1              5              1             14
INI                               1              4              0             14
--------------------------------------------------------------------------------
SUM:                           1031          12802          53894          76717
--------------------------------------------------------------------------------
		</pre></div>
	</div>
	</section>
	<section>
		<h2>NWB: light as a feather, strong as an ox</h2>
		<p>High-performance, well-defined data models compatible with arbitrary array backends with three dependencies.
			An overlay format that takes data as it comes. </p>
		<pre><code data-trim class="language-yaml">
meta:
  id: my_dataset
	# Import schemas from anywhere, local files, pypi, URIs, whatevs
	imports:
    core:
      as: nwb
      version: "2.7.0"
      from:
        - pypi: nwb-models
				- uri: https://example.com/nwb

# mix and match data and schemas
data: !nwb.NWBFile
	identifier: "blake2b:hashhashhash"
	acquisition:
		running_wheel_rotation: !nwb.TimeSeries
			hdf5:
				file: my_data/file.h5
				path: /dataset/within/file
				hash: blake2b:hashahsh
		my_ephys: !nwb.ElectricalSeries
			zarr: my_zarr_array.zarr
			hash: blake2b: hashhashfuiehwf
	stimulus:
		mouse_movies:
			video: the_matrix_1999.mp4




		</code></pre>
	</section>
</section>
<section class="center">
	<h1>Pause for reflection on integration with rest of lab projects...</h1>

</section>

				<section>
					<div class="row">
						<div class="column half"></div>
						<div class="column half"></div>
					</div>
				</section>
				<section>
					<div class="row">
						<div class="column half"></div>
						<div class="column half"></div>
					</div>
				</section>



			</div>
		</div>

		<script src="reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script src="plugin/menu/menu.js"></script>
		<script src="plugin/reveald3/reveald3.js"></script>
		<script src="plugin/mermaid/mermaid.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
				transition: 'fade',
				transitionSpeed: 'fast',
				disableLayout: true,

				mermaid: {
					htmlLabels: false,
					flowchart: {
						padding: 30
					}
				},

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMenu, Reveald3, RevealMermaid ]
			});
		</script>
	</body>
</html>
